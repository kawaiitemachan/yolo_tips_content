# YOLOを使った顔検出とモザイク処理コンテンツへようこそ！

今回のコンテンツでは、物体検出技術「YOLO (You Only Look Once)」を使って、画像から人の顔を検出し、その顔にモザイクをかけるプログラムを作成する手順を、ステップバイステップで丁寧に解説します。

## このコンテンツの対象読者

特に以下のような方々を対象としています。

*   プログラミングを始めたばかりの初心者の方
*   画像処理、機械学習は初めてという方
*   物体検出＆モザイクに興味はあるけれど、どこから手をつけていいかわからない方
*   具体的なプロジェクトを通して、実践的なスキルを身につけたい方

専門的な知識は不要です。
マウス操作を中心とした解説から始まり、徐々にプログラムの仕組みにも触れていきますので、ご安心くださいませ！

## このコンテンツで学べること

このコンテンツを最後まで進めることで、

*   クラウドベースのプラットフォーム「Roboflow」を使って、顔検出のためのカスタムデータセットを作成し、アノテーション（学習の素材となる教師データ作成）を行う方法を理解できます。
*   作成したデータセットを使って、YOLOモデルのトレーニングを行い、自分だけの顔検出モデルを構築する流れを把握できます。
*   構築したYOLOモデルとPythonプログラムを連携させ、画像内の顔を自動的に検出し、モザイク処理を施すプログラムを実行できるようになります。
*   一連の作業を通して、機械学習プロジェクトの基本的な進め方や、YOLOのような先進的な技術を実際に活用する面白さを体験できます。

## 2. Roboflowアカウント作成とプロジェクト設定：顔検出モデル作成の第一歩

ここからは、実際に手を動かしながら、顔検出モデルを作成するための準備を始めていきましょう！
最初のステップは、モデル構築の際に便利なプラットフォーム「Roboflow」のアカウントを作成し、プロジェクトを設定することです。

### Roboflowとは？

Roboflowは、画像や動画データセットの管理、アノテーション（機械学習モデルに「何を検出するか」を教えるためのラベル付け作業）、モデルのトレーニング、そしてデプロイ（実運用環境への展開）までをサポートしてくれる、非常に強力なウェブベースのプラットフォームです！
特に、YOLOのような物体検出モデルを扱う際には、データセットの準備や管理が煩雑になりがちですが、Roboflowを使うことでこれらの作業を効率的に進めることができます。
プログラミングの知識がなくても、直感的に操作できるため、初心者の方でも安心して利用できます！

### アカウント作成と最初の設定

それでは、早速Roboflowのウェブサイトにアクセスし、アカウントを作成してみましょう。
https://roboflow.com/

### 画像1：ログイン後の承認画面

（ここに画像1「ログイン後の承認画面.png」を挿入）

この画面が表示されたらチェックボックスにチェックを入れてContinue


### 画像2：ワークスペース作成

（ここに画像2「ワークスペース作成.png」を挿入）

次に、作業を行うための「ワークスペース」を作成します！
ワークスペースは、複数のプロジェクトを管理するための大きな箱のようなものです。
上の画像のような画面が表示されたら、以下の手順で進めます。

1.  **プロジェクトの名前を入力（なんでもOKです）**: まずはワークスペース（または最初のプロジェクト）の名前を決めます。今回は顔検出のプロジェクトなので、「FaceDetection」や「MyFaceProject」など、わかりやすい名前を自由につけてみましょう！
2.  **ひとまず無料OK**: Roboflowには有料プランもありますが、個人利用や学習目的であれば無料プランで十分な機能を利用できます。画像内で示されているように、無料プランに該当するPublic Planを選びましょう。
3.  **ここを押して次に進む**: 名前の入力とプランの選択が終わったら、画面の指示に従って「Continue」のボタンをクリックして、ワークスペースの作成を完了します。

### 画像3：他の編集者の招待（何も入力せずに次へ）

（ここに画像3「他の編集者の招待(何も入力せずに次へ).png」を挿入）

ワークスペース作成後、他のユーザーを編集者として招待する画面が表示されることがあります。
これは、チームで共同作業を行う際に便利な機能ですが、今回は個人で進めるため、特に何も入力する必要はありません。
何も入力せずに「Next」や「Skip」といったボタンをクリックして、このステップをスキップしましょう。
(もちろん他の編集者を追加したい場合は権限を決めて招待でOKです)

### 画像4：新規プロジェクトの作成

（ここに画像4「新規プロジェクトの作成.png」を挿入）

ワークスペースの準備が整ったら、いよいよ顔検出モデルを作成するための「プロジェクト」を作成します。
上の画像のように、「Projects」のメニュー項目を探し、クリックします。
その後、「New Project」をクリックし、新しいプロジェクトの作成を開始します。

### 画像5：プロジェクトの詳細の決定

（ここに画像5「プロジェクトの詳細の決定.png」を挿入）

新しいプロジェクトを作成する際には、いくつかの詳細設定を行う必要があります。
上の画像を参考に、以下の情報を入力・選択していきましょう！

1.  **プロジェクト名を入力（なんでもOKです）**: ここでも、プロジェクトに名前をつけます。なんでもいいのですが、今後の他のモデルを作ることも考えると、先ほどワークスペースにつけた名前と関連性のあるものや、より具体的な名前（例：「MyFaceDetectionModel」）などにしておきましょう！
2.  **Instance Segmentationを選択**: プロジェクトの種類を選択する項目です。YOLOを使った物体検出にはいくつかの種類がありますが、今回は顔の「輪郭」を捉えたいので、「Instance Segmentation（インスタンスセグメンテーション）」またはそれに類する選択肢を選びます。Instance Segmentationは、物体の位置だけでなく、その形状までをピクセル単位で特定するタスクです。これにより、より精密な顔検出が可能になります。
3.  **プロジェクトを作成します**: プロジェクト名とプロジェクトの種類を選択したら、「Create Project」のボタンをクリックして、プロジェクトの作成を完了します。

これで、Roboflow上に顔検出モデル開発のためのプロジェクトが作成されました！
次のステップでは、このプロジェクトに顔が写っている画像をアップロードし、アノテーション作業を行っていきます。


### 画像6：アノテーション画像のアップロード

（ここに画像6「アノテーション画像のアップロード.png」を挿入）

プロジェクトの準備が整ったら、次はいよいよ顔検出モデルに学習させるための画像をアップロードします。
画像は、Roboflowで画像をアップロードする際の画面です。

1.  **ドラッグ＆ドロップでファイルをアップロード**: 最も簡単な方法は、お使いのコンピュータから画像ファイルを直接画面上にドラッグ＆ドロップすることです。複数の画像ファイルを一度に選択してドラッグ＆ドロップすることも可能です。
2.  **Select Filesから画像を選択**: 画面上に「Select Files」や「ファイルを選択」といったボタンがあれば、それをクリックすることでファイル選択ダイアログが開き、個別の画像ファイルを選択してアップロードできます。
3.  **Select Folderから画像が格納されたフォルダを選択**: もし「Select Folder」や「フォルダを選択」といったボタンがあれば、画像がまとめて保存されているフォルダごと選択してアップロードすることもできます。これにより、大量の画像を一度に効率よくアップロードできます。

どの方法を使っても構いませんので、顔が写っている画像をいくつか選んでアップロードしてみましょう。
様々な角度、表情、背景の顔画像を用意すると、より性能の高い顔検出モデルを作成できます。

### 画像7：アップロード＆保存

（ここに画像7「アップロード＆保存.png」を挿入）

画像のアップロードが完了すると、上の画像のような確認画面が表示されます。
アップロードされた画像の一覧や枚数などが表示されますので、内容を確認してください。
問題がなければ、画像内のメモ書きの指示通り、「Save and Continue」のボタンをクリックして、次のステップへ進みます。

### 画像8：マニュアルでのラベリングを選択

（ここに画像8「マニュアルでのラベリングを選択.png」を挿入）

画像をアップロードした後、次に「ラベリング」という作業を行います。
ラベリングとは、画像の中の「どこに何が写っているか」をモデルに教えるための作業です。
今回は顔を検出したいので、画像の中の顔の部分を囲んで「これは顔です」と教えてあげる必要があります。

上の画像は、ラベリングの方法を選択する画面の一例です。
Roboflowには自動でラベリングを補助してくれる機能もありますが、今回は基本的な手順を学ぶために、手動でラベリングを行います。
画像内のメモ書きで示されているように、「Start Manual Labeling」の選択肢を選びましょう。

### 画像9：アノテーション担当を選択＆アサイン

（ここに画像9「アノテーション担当を選択＆アサイン.png」を挿入）

次に、アノテーション作業（ラベリング作業のことです）を担当するユーザーを選択する画面が表示されます。
チームで作業している場合は、ここで担当者を割り当てることができます。

今回は個人で作業しているので、以下の手順で進めます。

1.  **自分のアカウントを選択**: 画面に表示されているユーザーリストの中から、ご自身のアカウント名を探して選択します。
2.  **Assign to Myselfを押す**: アカウントを選択したら、「Assign to Myselfボタンを押します。

### 画像10：アノテーションのスタートボタン

（ここに画像10「アノテーションのスタートボタン.png」を挿入）

アノテーション担当者を設定すると、いよいよアノテーション作業を開始するための画面に遷移します。
上の画像のように、アップロードした画像の一覧が表示され、「Start Annotating」ボタンが表示されているはずです。
画像内のメモ書きの指示通り、このボタンをクリックして先に進みましょう！

### 画像11：先頭の画像を選択してアノテーションスタート

（ここに画像11「先頭の画像を選択してアノテーションスタート.png」を挿入）

選択画面にはアップロードした画像が表示されているはずです。
まずは、アノテーションを開始する画像を選びましょう。
画像内のメモ書きの指示通り、一覧の中から先頭の画像をクリックして選択します。

### 画像12：ポリゴンツールを選択する

（ここに画像12「ポリゴンツールを選択する.png」を挿入）

アノテーションを行う画像を選択したら、次にアノテーションに使用するツールを選びます。
今回は顔の「輪郭」に沿って精密にアノテーションを行いたいため、「ポリゴンツール（Polygon Tool）」を使用します。
ポリゴンツールは、複数の点をクリックして多角形を描くことで、複雑な形状のオブジェクトも囲むことができるツールです。

画面の右側に、様々なアノテーションツールがアイコンで表示されているはずです。
上の画像内のメモ書きで示されているように、ポリゴンツールのアイコンを探してクリックし、選択してください。

### 画像13：アノテーション＆クラス決め＆TRAINに割り振り

（ここに画像13「アノテーション＆クラス決め＆TRAINに割り振り.png」を挿入）

ポリゴンツールを選択したら、いよいよ画像内の顔にアノテーションを施していきます。
上の画像は、アノテーション作業中の画面の一例になります！

1.  **顔の輪郭に沿って点を打っていく**: マウスを使って、検出したい顔の輪郭に沿って、細かく点をクリックしていきます。点をたくさん打つほど、より正確な輪郭で顔を囲むことができます。最初の点をクリックした後、次の点をクリックすると線が引かれ、これを繰り返して顔全体を囲みます。最初の点にカーソルを合わせると１つの囲みになります。
2.  **今回は顔の検出なので「face」と入力（手の場合は「hand」など）**: ポリゴンで顔を囲むと、その囲んだ領域が何であるかを示す「クラス名」を入力するポップアップウィンドウが表示されます。今回は顔を検出したいので、クラス名として「face」と入力します。もし手の検出を行いたい場合は「hand」と入力するなど、検出対象に応じて適切なクラス名を設定します。クラス名は、モデルが何を学習すべきかを定義する重要な情報になります。次回以降は作成したクラスは選択項目に表示されるようになります。
3.  **最後にSaveを押す**: クラス名を入力したら、「Save」ボタンをクリックして、アノテーションを保存します。これで、1つの顔に対するアノテーションが完了です。1枚の画像に複数の顔がある場合は、それぞれの顔に対して同様の作業を繰り返します。
4.  **もし「TEST」になっていたら「TRAIN」に変更**: 画像の右上に、現在編集中の画像が「TRAIN（学習用）」「VALID（検証用）」「TEST（テスト用）」のどれに割り当てられているかを示す表示があります。アノテーション作業中は、基本的に画像を「TRAIN」に割り当てるようにしましょう。もし「TEST」や「VALID」になっている場合は、クリックして「TRAIN」に変更してください。（データセットの分割については後ほど詳しく説明します）

### 画像14：次の画像に移動

（ここに画像14「次の画像に移動.png」を挿入）

1枚の画像に対するアノテーション作業がすべて完了したら、次の画像に進みます。
アノテーションエディタの上部に、次の画像や前の画像に移動するための矢印ボタン（「>」や「<」など）があります。
上の画像内のメモ書きの指示通り、右向きの矢印ボタンをクリックして、次の画像に移動し、同様にアノテーション作業を続けていきましょう。
アップロードしたすべての画像に対して、丁寧にアノテーションを行っていきましょう！


## 4. データセット作成とダウンロード：モデル学習の準備を整える

すべてのアノテーション作業が完了したら、次はいよいよ学習用データセットを作成し、ローカル環境にダウンロードします。
このデータセットが、YOLOモデルが「顔とは何か」を学習するための教科書となります！

### 画像15：データセットへの追加

（ここに画像15「データセットへの追加.png」を挿入）

すべてのアノテーション済み画像を確認し終えたら、これらの画像をデータセットに追加します。
上の画像は、アノテーションが完了した画像の一覧画面を示しています。

1.  **Annotatedを押す**: まず、画面上部にある「Annotated」タブが選択されていることを確認します。これにより、アノテーションが完了した画像のみが表示されます。
2.  **Add X images to Datasetを押す**: アノテーション済みの画像がすべて表示されたら、それらをデータセットに追加するためのボタン（例：「Add X Images to Dataset」や「データセットに追加」）をクリックします。Xにはアノテーション済みの画像の枚数が入ります。

### 画像16：Train＆Validの比率の選択

（ここに画像16「Train＆Validの比率の選択.png」を挿入）

データセットに追加する画像を選択すると、次にその画像をどのように分割するかを設定する画面が表示されます。
機械学習モデルを訓練する際には、一般的にデータを「学習用（Train）」「検証用（Valid）」「テスト用（Test）」の3つに分割します。

*   **学習用（Train）**: モデルが直接学習するために使用されるデータです。
*   **検証用（Valid）**: 学習中にモデルの性能を評価し、過学習（学習データに過剰に適合しすぎて、未知のデータに対する性能が低下すること）を防いだり、ハイパーパラメータ（学習率など）を調整したりするために使用されます。
*   **テスト用（Test）**: 学習が完了したモデルの最終的な性能を評価するために使用される、完全に未知のデータです。

上の画像は、このデータ分割の比率を設定する画面です。
今回は、以下のように設定しましょう！

1.  **画像内の選択肢を選ぶ**: データ分割の方法を選択します。選択項目から「Split Images Between Train/Valid/Test」を選びます。
2.  **Train：80%、Valid：20%に変更**: スライダーを操作して、学習用データ（Train）の割合を80%、検証用データ（Valid）の割合を20%に設定します。テスト用データ（Test）は今回は0%で構いません。これは、比較的小規模なデータセットで、まずは基本的なモデルを作成することを目的としているためです。（より大規模なプロジェクトでは、テスト用データも適切に割り当てることも重要です）
3.  **Add Imagesを押す**: 比率の設定が完了したら、「Add Images」や「画像を追加」といったボタンをクリックして、データセットへの画像の追加と分割を確定します。

### 画像17：新しいデータセットの作成準備

（ここに画像17「新しいデータセットの作成準備.png」を挿入）

Roboflowでは、データセットに対する変更（アノテーションの修正、画像の追加・削除、前処理の変更など）をバージョンとして管理できます。
これにより、過去のデータセットの状態に戻したり、異なるバージョンのデータセットでモデルの性能を比較したりすることが容易になります！

上の画像は、新しいデータセットバージョンを作成するための準備画面です。

1.  **Datasetのメニューに移動**: まず、左側のメニューから「Dataset」が選択されていることを確認します。
2.  **New Dataset Versionを押す**: 画面上部にある「New Dataset Version」のボタンをクリックして、新しいバージョンの作成を開始します。

### 画像18：データセット作成準備

（ここに画像18「データセット作成準備.png」を挿入）

新しいデータセットバージョンを作成する過程で、いくつかの設定ステップがあります。
最初のステップは「Preprocessing（前処理）」です。前処理とは、モデルの学習効率や性能を向上させるために、元画像に対して行われる様々な変換処理のことです。
例えば、画像の向きを自動で修正したり、画像のサイズを統一したりします。

上の画像は、前処理の設定画面です。

1.  **画像の表示と一致していることを確認**: 通常、Roboflowはプロジェクトの種類やデータの内容に応じて、推奨される前処理ステップを自動的に提案してくれます。「Auto-Orient（自動方向修正）」や「Resize（リサイズ、例：Stretch to 640x640）」といった項目が設定されていることを確認しましょう。
2.  **Continueを押す**: 設定内容を確認したら、「Continue」をクリックして、次のステップに進みます。

### 画像19：データセット作成準備２（Augmentation設定）

（ここに画像19「データセット作成準備２.png」を挿入）

前処理の次のステップは「Augmentation（データ拡張）」です。
データ拡張とは、既存の学習データにランダムな変換（回転、反転、明るさ調整など）を加えることで、学習データのバリエーションを人工的に増やし、モデルの汎化性能（未知のデータに対する適応能力）を高めるテクニックです。
特に、学習データが少ない場合に有効です。

上の画像は、データ拡張の設定画面です。

1.  **Continueを押す**: 「Continue」をクリックして、このステップはそのままスキップ（またはデフォルトのオフ状態）で進めて大丈夫です。YOLOのモデルはトレーニング時に自動で様々なオーグメンテーション（データ拡張）処理を行ってくれるため、ここで特別な設定をしなくても十分に高い汎化性能が期待できます。

### 画像20：データセット作成準備３（データセット作成実行）

（ここに画像20「データセット作成準備３.png」を挿入）

前処理とデータ拡張の設定（今回は拡張なし）が完了すると、いよいよデータセットバージョンを作成する最終ステップです！

上の画像は、これまでの設定内容の確認と、データセット作成を実行するための画面です。

1.  **Createを押す**: 内容を再確認して「Create」をクリックすると、Roboflowがこれまでの設定に基づいてデータセットの生成を開始します。画像の枚数や処理内容によっては、少し時間がかかる場合があります。

### 画像21：データセットのダウンロード

（ここに画像21「データセットのダウンロード.png」を挿入）

データセットの生成が完了すると、上の画像のような画面が表示されます。

1.  **新しいデータセットが作成されていることを確認**: 画面左側や中央に、新しく作成されたデータセットのバージョン情報（例：「v1 - 2025-05-09 11:30am」など）が表示されていることを確認します。
2.  **Download Datasetを押す**: 作成されたデータセットをローカル環境にダウンロードするために、「Download Dataset」をクリックします。

### 画像22：ZIP形式でデータセットをダウンロード

（ここに画像22「ZIP形式でデータセットをダウンロード.png」を挿入）

データセットのダウンロードボタンをクリックすると、ダウンロード形式を選択するポップアップウィンドウが表示されます。
YOLOモデルのトレーニングに使用するためには、特定のフォーマットでデータセットをダウンロードする必要があります。

上の画像は、ダウンロード形式の選択画面です。

1.  **Formatから「YOLOv11」を選択**: 「Format」というドロップダウンリストから、今回は「YOLOv11」を選択します。
2.  **ZIP形式でのダウンロードを選択**: ダウンロードオプションとして、「Download zip to computer」を選択して一括ダウンロードする選択肢を選びます。
3.  **Continueを押す**: フォーマットとダウンロードオプションを選択したら、「Continue」をクリックします。すると、データセットがZIPファイルとしてダウンロードされます。

これで、顔検出モデルをトレーニングするためのデータセットの準備とダウンロードが完了しました！
次の章では、ダウンロードしたデータセットを使って、いよいよローカル環境でYOLOモデルのトレーニングを行います。


## 5. ローカル環境でのセットアップとモデルトレーニング：自分だけの顔検出モデルを鍛え上げる

Roboflowから顔検出用のデータセットをダウンロードしたら、次はいよいよあなたのコンピュータ（ローカル環境）で、YOLOモデルのトレーニングを行います。
このステップで、ダウンロードしたデータセットを使って、AIに「顔とはどのようなものか」を学習させ、あなた専用の顔検出モデルを構築していきましょう！

### 画像23：ZIPフォルダの展開

（ここに画像23「ZIPフォルダの展開.png」を挿入）

まずは、先ほどRoboflowからダウンロードしたデータセットのZIPファイルを展開（解凍）します。

### 画像24：展開後のフォルダ名をdatasetに変更

（ここに画像24「展開後のフォルダをdatasetに変更.png」を挿入）

ZIPファイルを展開すると、データセット名のフォルダ（例：「FaceDetection-1」など、Roboflowで設定したプロジェクト名とバージョン番号に基づいた名前）が作成されます。
このフォルダ名を、後の作業で使いやすくするために「**dataset**」という名前に変更します。上の画像のように、展開されたフォルダを選択し、名前を変更してください。

### 画像25：フォルダ構成＆datasetの入れ替え と 画像31：train-validのデータセットの入れ替え作業

（ここに画像25「フォルダ構成＆datasetの入れ替え.png」を挿入）

**作業手順：**
1. Roboflowからデータセットをダウンロードし、ZIPファイルを展開します。
2. 展開後のフォルダの名前を「dataset」に変更します。
3. この「dataset」フォルダを、今回のコンテンツに含まれている`training_folder`の中にある既存の「dataset」フォルダと入れ替えます（上書きまたは削除してからコピーしてください）。

### Windows環境でのPython環境構築の準備

モデルのトレーニングや後述するモザイク処理プログラムを実行するためには、Pythonとその関連ライブラリ（プログラム部品）が必要です。
リポジトリには、これらの環境を簡単にセットアップするためのバッチファイル（`.bat`拡張子のファイル）が含まれています。

**補足：Pythonのインストール**
もし、お使いのWindows環境にPythonがインストールされていない場合は、事前にPython公式サイトからインストーラをダウンロードし、インストールしておく必要があります。
その際、「Add Python to PATH」のチェックボックスをオンにすることを忘れないでください。
YOLOv11やUltralyticsライブラリは比較的新しいPythonバージョンを要求することがあるため、Python 3.11以上（3.12付近を推奨）をインストールすると良いでしょう。
（Python3.12公式リンク：https://www.python.org/ftp/python/3.12.10/python-3.12.10-amd64.exe）

### 画像26：環境設定ファイルの起動 と 画像29：仮装環境作成

（ここに画像26「環境設定ファイルの起動.png」を挿入）
（ここに画像29「仮装環境作成.png」を挿入）

`training_folder`の中に、「`setup_environment.bat`」というファイルがあります。
これが、Pythonの仮想環境を作成し、必要なライブラリをインストールしてくれるスクリプトです。

1.  **`setup_environment.bat`をダブルクリックして実行します。**
    すると、コマンドプロンプト（黒い画面）が起動し、自動的に処理が始まります。画像29は、このスクリプトが実行されている途中の様子を示しています。「Requirement already satisfied」というメッセージが多く表示されることがありますが、これは必要なライブラリが既に存在しているか、依存関係で解決されていることを示しています。初めて実行する場合は、ライブラリのダウンロードとインストールが行われるため、少し時間がかかることがあります。
2.  **「Environment setup complete!」と表示されるまで待機します。**
    画像29の下部にあるように、「Environment setup complete!」というメッセージが表示され、「続行するには何かキーを押してください...」と表示されたら、環境構築は成功です。
3.  **Enterキーを押してコマンドプロンプトを閉じます。**

このスクリプトは、`venv`という名前のPython仮想環境を`training_folder`内に作成し、`ultralytics`（YOLOの実行に必要）や`opencv-python`（画像処理に必要）などのライブラリをその仮想環境内にインストールします。
仮想環境を使うことで、プロジェクトごとに異なるバージョンのライブラリを管理でき、他のPython環境との衝突を避けることができます。

### `config.yaml`ファイルの詳細解説

`training_folder`の中には、「`config.yaml`」というファイルがあります。
これは、YOLOモデルのトレーニング設定を記述する構成ファイルです。
（気になる方は中身をテキストエディタで開いて確認してみてください）

```yaml
# YOLOv11n-seg Training Configuration

# Model parameters
model: yolo11n-seg.pt  # ベースとする事前学習済みモデル。yolov11n-seg.ptは軽量なセグメンテーションモデルです。
epochs: 100            # 学習を繰り返す回数（エポック数）。データセットの規模や複雑さに応じて調整します。
imgsz: 640             # 学習に使用する画像のサイズ。640x640ピクセルにリサイズされます。
batch: 4               # 一度に処理する画像の枚数（バッチサイズ）。GPUメモリに応じて調整します。
workers: 4             # データ読み込みに使用するCPUコア数。環境に合わせて調整します。
device: 0              # 使用するGPUのデバイス番号。0は通常、最初のGPUを指します。CPUのみの場合は 'cpu' と指定します。

# Dataset
data: ./dataset/data.yaml # データセットの構成ファイル（Roboflowからダウンロードしたもの）へのパス。

# Output
name: yolov11n_seg_custom_model # 学習結果が保存されるフォルダ名。
project: runs/segment          # 学習結果が保存されるプロジェクトディレクトリ。
```

*   **`model`**: トレーニングのベースとなるモデルを指定します。`yolo11n-seg.pt`は、YOLOv11のnanoバージョンで、セグメンテーション（物体の輪郭検出）に対応した軽量モデルです。`.pt`ファイルはPyTorch形式の学習済みモデルファイルです。
*   **`epochs`**: 学習データセット全体を何回繰り返して学習するかを指定します。エポック数が多いほど学習が進みますが、多すぎると過学習のリスクもあります。通常は数十から数百の範囲で設定されます。
*   **`imgsz`**: 入力画像をどのサイズにリサイズして学習に使用するかを指定します。YOLOモデルは特定の入力サイズを期待することが多いです。
*   **`batch`**: 一度の計算でモデルに渡す画像の枚数です。バッチサイズが大きいほど学習が安定する傾向がありますが、GPUのメモリ容量を多く消費します。メモリ不足エラーが出る場合は、この値を小さくします。
*   **`workers`**: データ読み込み処理を並列で行うためのワーカースレッド数です。CPUのコア数に応じて設定します。
*   **`device`**: 学習に使用するデバイスを指定します。NVIDIAのGPUが利用可能な場合は`0`のように指定します。GPUがない場合は`cpu`と指定します。
*   **`data`**: データセットの構成ファイルである`data.yaml`へのパスを指定します。この`data.yaml`は、Roboflowからダウンロードした`dataset`フォルダ内に含まれており、学習用・検証用画像のパスやクラス名などが定義されています。
*   **`name`**: 学習結果（学習済みモデルの重みファイルやログなど）が保存されるフォルダの名前です。
*   **`project`**: `name`で指定したフォルダが作成される親ディレクトリです。

これらのパラメータは、必要に応じて調整することができますが、まずはデフォルトの設定でトレーニングを実行してみましょう。

### 画像27：トレーニング用のファイルの起動 と 画像30：モデルトレーニング

（ここに画像27「トレーニング用のファイルの起動.png」を挿入）
（ここに画像30「モデルトレーニング.png」を挿入）

環境設定とデータセットの準備が整ったら、いよいよモデルのトレーニングを開始します。`training_folder`の中に、「`train_yolov11n_seg.bat`」というファイルがあります。これが、`config.yaml`の設定に基づいてYOLOモデルのトレーニングを実行するスクリプトです。

1.  **`train_yolov11n_seg.bat`をダブルクリックして実行します。**
    コマンドプロンプトが起動し、トレーニングプロセスが開始されます。画像30は、トレーニングが進行している様子を示しています。GPUが利用可能な場合は、GPUを使って高速に学習が行われます。
    画面には、エポックごとの学習状況（損失関数の値、精度など）が表示されます。
    ```
    Epoch   gpu_mem box_loss seg_loss cls_loss dfl_loss Instances Size
    1/100   0.506G   0.8321   0.6012    2.543   0.9876         2  640: 100% ...
    ... (中略) ...
    Epoch   gpu_mem box_loss seg_loss cls_loss dfl_loss Instances Size
    100/100 0.506G   0.3569   0.4097     1.81   0.8024         2  640: 100% ...
    ```
    これらの数値は、モデルがどれだけうまく学習できているかを示す指標です。通常、`loss`（損失）の値はエポックが進むにつれて小さくなっていきます。
2.  **トレーニングが完了するまで待ちます。**
    `config.yaml`で指定されたエポック数（デフォルトでは100エポック）の学習が完了するまでには、コンピュータの性能やデータセットのサイズによって数分から数時間かかることがあります。画像30では、100エポックの学習が完了した後のメッセージが表示されています。
    ```
    100 epochs completed in 0.012 hours.
    Optimizer stripped from runs/segment/yolov11n_seg_custom_modelX/weights/last.pt, 6.0MB
    Optimizer stripped from runs/segment/yolov11n_seg_custom_modelX/weights/best.pt, 6.0MB
    Validating runs/segment/yolov11n_seg_custom_modelX/weights/best.pt...
    (中略)
    Results saved to runs/segment/yolov11n_seg_custom_modelX
    Learn more at https://docs.ultralytics.com/modes/train
    Training completed.
    ```
    `yolov11n_seg_custom_modelX` の `X` の部分は実行ごとに数字が変わることがあります。
3.  **「続行するには何かキーを押してください...」と表示されたら、Enterキーを押してコマンドプロンプトを閉じます。**

トレーニングが完了すると、`training_folder`内の`runs/segment/yolov11n_seg_custom_modelX`（または`config.yaml`で指定した`project`/`name`）というフォルダの中に、学習結果が保存されます。特に重要なのは、`weights`フォルダの中にある`best.pt`というファイルです。これが、検証用データセットで最も良い性能を示した学習済みモデルの重みファイルであり、後の顔モザイク処理プログラムで使用します。

これで、あなただけのカスタム顔検出モデルのトレーニングが完了しました！次の章では、この学習済みモデルを使って、実際に画像から顔を検出し、モザイクをかけるプログラムを実行してみましょう。


## 6. 顔モザイクプログラムの実行：学習済みモデルで顔を隠そう！

カスタム顔検出モデルのトレーニングが完了し、`best.pt`という学習済みモデルファイルを手に入れました。
いよいよ最後のステップ、このモデルを使って実際に画像内の顔を検出し、モザイクをかけるプログラムを実行してみましょう。

### 画像28：モザイクプログラムの起動

（ここに画像28「モザイクプログラムの起動.png」を挿入）

`training_folder`の中に、「`detect_mosaic.bat`」というファイルがあります。
これが、学習済みのYOLOモデル（`best.pt`）を使い、指定されたフォルダ内の画像に含まれる顔を検出し、それらにモザイク処理を施して別のフォルダに保存するPythonプログラム（`detect.py`）を実行するためのバッチファイルです。

1.  **`detect_mosaic.bat`をダブルクリックして実行します。**
    コマンドプロンプトが起動し、Pythonプログラム`detect.py`が実行されます。

### `detect.py`プログラムの詳細解説

`detect_mosaic.bat`を実行すると、内部的には`python detect.py`というコマンドが実行され、`detect.py`スクリプトが動作します。
このスクリプトは、あなたにいくつかの情報を尋ね、それに基づいて処理を行います。

#### プログラムの機能概要

`detect.py`は、以下の機能を持っています。

*   指定されたフォルダ内にあるすべての画像ファイル（.jpg, .jpeg, .png, .bmp, .tiff）を読み込みます。
*   今回作成したトレーニング済みのYOLOv11n-segモデル（`runs/segment/yolov11n_seg_custom_modelX/weights/best.pt`）を使用して、各画像から顔の領域を検出（セグメンテーション）します。
*   検出された顔の領域に対して、指定されたサイズのモザイク処理を適用します。
*   モザイク処理が施された画像を、`runs/segment/mosaic_results`というフォルダに保存します。元画像と同じファイル名で、先頭に`mosaic_`という接頭辞が付きます。
*   もしモデルが見つからない場合は、代替パスの入力を促します。
*   GPUが利用可能であればGPUを、そうでなければCPUを使用して推論処理を行います。

#### ユーザー入力部分の説明

`detect.py`を実行すると、コマンドプロンプトで以下の情報を順番に入力するよう求められます。

1.  **Enter folder path containing images to process:**
    モザイク処理を施したい画像が保存されているフォルダのパスを入力します。例えば、デスクトップにある「TestImages」というフォルダを指定したい場合は、そのフルパス（例：`C:\Users\YourUserName\Desktop\TestImages`）を入力します。リポジトリ内の`training_folder`の兄弟ディレクトリとして`annotation_test_images`というフォルダが提供されているので、これを使ってテストすることができます。その場合のパスは、`training_folder`からの相対パスで`../annotation_test_images`のように指定するか、フルパスで指定します。

2.  **Enter mosaic pixel size (smaller = more pixelated, recommended 8-20):**
    モザイクの粗さを決定するピクセルサイズを入力します。この値が小さいほど、モザイクはより細かく（ピクセルが小さく）なり、元の顔が分かりにくくなります。逆に大きいと、モザイクは粗くなります。推奨値は8から20の間です。例えば、「10」と入力します。不正な値を入力した場合は、デフォルトで10が使用されます。

3.  **Enter detection confidence threshold (0.1-1.0, default=0.25):**
    顔検出の信頼度の閾値を入力します。モデルが顔を検出した際に、その確からしさ（信頼度スコア）がこの閾値以上である場合にのみ、モザイク処理の対象となります。値は0.1から1.0の間で設定します。例えば、「0.25」と入力すると、25%以上の信頼度で検出された顔にモザイクがかかります。この値を高くすると、誤検出は減りますが、検出漏れが増える可能性があります。低くすると、より多くの顔を検出しようとしますが、顔でないものを誤って検出する可能性も高まります。不正な値を入力した場合は、デフォルトで0.25が使用されます。

#### 各入力値の推奨値や設定方法の具体例

*   **画像フォルダパス**: まずはリポジトリ内の`annotation_test_images`フォルダを使ってみましょう。もし`training_folder`が `C:\MyProjects\yolo_tips_content\training_folder` にあるなら、テスト画像フォルダのフルパスは `C:\MyProjects\yolo_tips_content\annotation_test_images` となります。
*   **モザイクピクセルサイズ**: 「10」や「15」あたりから試してみて、結果を見ながら調整もしてみましょう！
*   **信頼度閾値**: デフォルトの「0.25」で試してみて、もし顔が検出されすぎる（顔でないものまで検出する）場合は少し値を大きく（例：0.4）、逆に顔の検出漏れが多い場合は少し値を小さく（例：0.15）してみると良いかもしれません。

#### プログラムの主要な処理の流れ

1.  学習済みモデル（`best.pt`）をロードします。
2.  指定された画像フォルダから画像ファイルを1枚ずつ読み込みます。
3.  読み込んだ画像に対してモデルを実行し、顔のセグメンテーションマスク（顔の領域を示すデータ）を取得します。
4.  取得した各セグメンテーションマスクに対して、`apply_mosaic_to_mask`関数を使ってモザイク処理を適用します。
    *   この関数は、マスク領域を一度縮小し、再度元のサイズに拡大することでモザイク効果を生み出します。
5.  モザイク処理後の画像を`runs/segment/mosaic_results`フォルダに保存します。
6.  すべての画像の処理が完了したら、その旨を通知してプログラムを終了します。

#### 出力結果（`mosaic_results`フォルダ）の説明

処理が完了すると、`training_folder`内の`runs/segment/mosaic_results`というフォルダが作成され、その中にモザイク処理済みの画像が保存されます。
ファイル名は、元の画像ファイル名の先頭に`mosaic_`が付加されたものになります（例：元の画像が`image1.jpg`なら、出力は`mosaic_image1.jpg`）。

### `annotation_test_images`フォルダの画像の利用方法

今回の配布フォルダには、`annotation_test_images`というフォルダが含まれています。
このフォルダには、アノテーション作業や、トレーニング済みモデルのテストに使用できるサンプル画像が数枚入れておきました！

顔モザイクプログラムを実行する際に、処理対象の画像フォルダとしてこの`annotation_test_images`フォルダのパスを指定することで、手軽にプログラムの動作確認や、作成した顔検出モデルの性能テストを行うことができます。

上記の流れに沿って実施していくことで 、YOLOを使った顔検出とモザイク処理の一連の流れを体験することができるかと思います！
今回学習したYOLOの物体検出モデルの作成は様々なことに応用ができる技術になります！

ぜひいろいろな学習データを作り、モデルをトレーニングしてみてくださいね！