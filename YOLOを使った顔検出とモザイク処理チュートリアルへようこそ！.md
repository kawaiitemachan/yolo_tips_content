# YOLOを使った顔検出とモザイク処理チュートリアルへようこそ！

このチュートリアルでは、近年注目を集めている物体検出技術「YOLO (You Only Look Once)」を使って、画像や動画の中から人の顔を検出し、その顔にモザイクをかけるプログラムを作成する手順を、ステップバイステップで丁寧に解説します。

## このチュートリアルの対象読者

このチュートリアルは、特に以下のような方々を対象としています。

*   プログラミングを始めたばかりの初心者の方
*   Pythonの基本的な知識はあるけれど、画像処理や機械学習は初めてという方
*   YOLOやUltralyticsといった技術に興味はあるけれど、どこから手をつけていいかわからない方
*   具体的なプロジェクトを通して、実践的なスキルを身につけたい方

専門的な知識は一切不要です。マウス操作を中心とした解説から始まり、徐々にプログラムの仕組みにも触れていきますので、安心して学習を進めてください。

## このチュートリアルで学べること

このチュートリアルを最後まで進めることで、あなたは以下のことができるようになります。

*   クラウドベースのAIプラットフォーム「Roboflow」を使って、顔検出のためのカスタムデータセットを作成し、アノテーション（教師データ作成）を行う方法を理解できます。
*   作成したデータセットを使って、YOLOモデルのトレーニングを行い、自分だけの顔検出モデルを構築する流れを把握できます。
*   構築したYOLOモデルとPythonプログラムを連携させ、画像内の顔を自動的に検出し、モザイク処理を施すプログラムを実行できるようになります。
*   一連の作業を通して、機械学習プロジェクトの基本的な進め方や、YOLOのような先進的な技術を実際に活用する面白さを体験できます。

さあ、一緒にYOLOを使った顔検出とモザイク処理の世界を探求していきましょう！


## 2. Roboflowアカウント作成とプロジェクト設定：顔検出モデル作成の第一歩

ここからは、実際に手を動かしながら、顔検出モデルを作成するための準備を始めましょう。最初のステップは、AI開発のための便利なプラットフォーム「Roboflow」のアカウントを作成し、プロジェクトを設定することです。

### Roboflowとは？

Roboflowは、画像や動画データセットの管理、アノテーション（機械学習モデルに「何を検出するか」を教えるためのラベル付け作業）、モデルのトレーニング、そしてデプロイ（実運用環境への展開）までをサポートしてくれる、非常に強力なウェブベースのプラットフォームです。特に、YOLOのような物体検出モデルを扱う際には、データセットの準備や管理が煩雑になりがちですが、Roboflowを使うことでこれらの作業を効率的に進めることができます。プログラミングの知識がなくても、直感的なインターフェースで操作できるため、初心者の方でも安心して利用できます。

### アカウント作成と最初の設定

それでは、早速Roboflowのウェブサイトにアクセスし、アカウントを作成してみましょう。通常、メールアドレスとパスワードで簡単に登録できます。Googleアカウントなどを使ったソーシャルログインにも対応している場合があります。

アカウント作成が完了し、ログインすると、最初の画面が表示されます。ここから、私たちの顔検出プロジェクトがスタートします。

### 画像1：ログイン後の承認画面

（ここに画像1「ログイン後の承認画面.png」を挿入）

上の画像は、Roboflowにログインした後に表示される可能性のある承認画面の一例です。もしこのような画面が表示された場合は、内容を確認し、問題がなければ承認ボタン（多くの場合、「Authorize」や「Approve」といった英語表記です）をクリックして次に進んでください。これは、RoboflowがあなたのGoogleアカウントなどの情報にアクセスすることを許可するための手続きです。特に心配する必要はありません。

もしこの画面が表示されずに、直接ダッシュボードやプロジェクト作成画面に進んだ場合は、このステップはスキップして問題ありません。


### 画像2：ワークスペース作成

（ここに画像2「ワークスペース作成.png」を挿入）

次に、Roboflowで作業を行うための「ワークスペース」を作成します。ワークスペースは、複数のプロジェクトを管理するための大きな箱のようなものだと考えてください。上の画像のような画面が表示されたら、以下の手順で進めます。

1.  **プロジェクトの名前を入力（なんでもOKです）**: 画像内のメモ書きにもある通り、まずはワークスペース（または最初のプロジェクト）の名前を決めます。今回は顔検出のプロジェクトなので、「FaceDetection」や「MyFaceProject」など、わかりやすい名前を自由につけてみましょう。日本語でも問題ありません。
2.  **ひとまず無料で進めるためこちらを選択**: Roboflowには有料プランもありますが、個人利用や学習目的であれば無料プランで十分な機能を利用できます。画像内で示されているように、無料プランやトライアルに該当する選択肢（例えば「Free Tier」や「Personal Use」など）を選びましょう。
3.  **ここを押して次に進む**: 名前の入力とプランの選択が終わったら、画面の指示に従って「Create Workspace」や「Next」といったボタンをクリックして、ワークスペースの作成を完了します。

### 画像3：他の編集者の招待（何も入力せずに次へ）

（ここに画像3「他の編集者の招待(何も入力せずに次へ).png」を挿入）

ワークスペース作成後、他のユーザーを編集者として招待する画面が表示されることがあります。これは、チームで共同作業を行う際に便利な機能ですが、今回は個人で進めるため、特に何も入力する必要はありません。画像内のメモ書きの指示通り、何も入力せずに「Next」や「Skip」といったボタンをクリックして、このステップをスキップしましょう。

### 画像4：新規プロジェクトの作成

（ここに画像4「新規プロジェクトの作成.png」を挿入）

ワークスペースの準備が整うと、いよいよ顔検出モデルを作成するための「プロジェクト」を作成します。多くの場合、ダッシュボード画面やプロジェクト一覧画面が表示されます。上の画像のように、「Projects」や「プロジェクト」といったメニュー項目を探し、クリックします。その後、「New Project」や「新しいプロジェクトを作成」といったボタンを見つけてクリックし、新しいプロジェクトの作成を開始します。

### 画像5：プロジェクトの詳細の決定

（ここに画像5「プロジェクトの詳細の決定.png」を挿入）

新しいプロジェクトを作成する際には、いくつかの詳細設定を行う必要があります。上の画像を参考に、以下の情報を入力・選択していきましょう。

1.  **プロジェクト名を入力する（なんでもOKです）**: ここでも、プロジェクトにわかりやすい名前をつけます。先ほどワークスペースにつけた名前と関連性のあるものや、より具体的な名前（例：「MyFaceDetectionModel」）などが良いでしょう。これも自由に決めて構いません。
2.  **今回は『顔の輪郭に沿って』検出を行いたいのでこちらを選択**: プロジェクトの種類を選択する項目です。YOLOを使った物体検出にはいくつかの種類がありますが、今回は顔の「輪郭」を正確に捉えたいので、「Instance Segmentation（インスタンスセグメンテーション）」またはそれに類する選択肢を選びます。Instance Segmentationは、物体の位置だけでなく、その形状までをピクセル単位で特定するタスクです。これにより、より精密な顔検出が可能になります。画像内のメモ書きで示されている選択肢をよく確認して選びましょう。もし「Object Detection (Bounding Box)」のような選択肢しかない場合は、そちらを選んでも基本的な顔検出は可能ですが、輪郭までの検出は行えません。可能であればInstance Segmentationを選択してください。
3.  **ここを押してプロジェクトを作成します**: プロジェクト名とプロジェクトの種類を選択したら、「Create Project」や「作成」といったボタンをクリックして、プロジェクトの作成を完了します。

これで、Roboflow上に顔検出モデル開発のためのプロジェクトが作成されました。次のステップでは、このプロジェクトに顔が写っている画像をアップロードし、アノテーション作業を行っていきます。


### 画像6：アノテーション画像のアップロード

（ここに画像6「アノテーション画像のアップロード.png」を挿入）

プロジェクトの準備が整ったら、次はいよいよ顔検出モデルに学習させるための画像をアップロードします。この作業を「アノテーション画像のアップロード」と呼びます。上の画像は、Roboflowで画像をアップロードする際の画面の一例です。いくつかの方法で画像をアップロードできます。

1.  **ドラッグ＆ドロップでファイルをアップロード**: 最も簡単な方法は、お使いのコンピュータから画像ファイルを直接画面上にドラッグ＆ドロップすることです。複数の画像ファイルを一度に選択してドラッグ＆ドロップすることも可能です。
2.  **Select Filesから画像を選択**: 画面上に「Select Files」や「ファイルを選択」といったボタンがあれば、それをクリックすることでファイル選択ダイアログが開き、個別の画像ファイルを選択してアップロードできます。
3.  **Select Folderから画像が格納されたフォルダを選択**: もし「Select Folder」や「フォルダを選択」といったボタンがあれば、画像がまとめて保存されているフォルダごと選択してアップロードすることもできます。これにより、大量の画像を一度に効率よくアップロードできます。

どの方法を使っても構いませんので、顔が写っている画像をいくつか選んでアップロードしてみましょう。様々な角度、表情、背景の顔画像を用意すると、より性能の高い顔検出モデルを作成できます。

### 画像7：アップロード＆保存

（ここに画像7「アップロード＆保存.png」を挿入）

画像のアップロードが完了すると、上の画像のような確認画面が表示されることがあります。アップロードされた画像の一覧や枚数などが表示されますので、内容を確認してください。問題がなければ、画像内のメモ書きの指示通り、「Save and Continue」や「保存して続行」、「Upload」といったボタンをクリックして、画像の保存と次のステップへ進みます。

### 画像8：マニュアルでのラベリングを選択

（ここに画像8「マニュアルでのラベリングを選択.png」を挿入）

画像をアップロードした後、次に「ラベリング」という作業を行います。ラベリングとは、画像の中の「どこに何が写っているか」をモデルに教えるための作業です。今回は顔を検出したいので、画像の中の顔の部分を囲んで「これは顔です」と教えてあげる必要があります。

上の画像は、ラベリングの方法を選択する画面の一例です。Roboflowには自動でラベリングを補助してくれる機能もありますが、今回は基本的な手順を学ぶために、手動でラベリングを行います。画像内のメモ書きで示されているように、「Manual Labeling」や「手動ラベリング」、「Annotate Manually」といった選択肢を選びましょう。

### 画像9：アノテーション担当を選択＆アサイン

（ここに画像9「アノテーション担当を選択＆アサイン.png」を挿入）

次に、アノテーション作業（ラベリング作業のことです）を担当するユーザーを選択する画面が表示されることがあります。チームで作業している場合は、ここで担当者を割り当てることができます。

今回は個人で作業しているので、以下の手順で進めます。

1.  **自分のアカウントを選択**: 画面に表示されているユーザーリストの中から、ご自身のアカウント名を探して選択します。
2.  **ここを押す**: アカウントを選択したら、「Assign」や「割り当て」、「Start Annotating」といったボタンをクリックして、アノテーション作業を開始します。

### 画像10：アノテーションのスタートボタン

（ここに画像10「アノテーションのスタートボタン.png」を挿入）

アノテーション担当者を設定すると、いよいよアノテーション作業を開始するための画面に遷移します。上の画像のように、アップロードした画像の一覧が表示され、「Start Annotating」や「アノテーション開始」といったボタンが表示されているはずです。画像内のメモ書きの指示通り、このボタンをクリックして、アノテーションエディタを開きましょう。

### 画像11：先頭の画像を選択してアノテーションスタート

（ここに画像11「先頭の画像を選択してアノテーションスタート.png」を挿入）

アノテーションエディタが開くと、アップロードした画像が表示されます。通常、左側や上部に画像の一覧（サムネイル）が表示され、中央に選択した画像が大きく表示されます。まずは、アノテーションを開始する画像を選びましょう。画像内のメモ書きの指示通り、一覧の中から先頭の画像（または任意の未アノテーション画像）をクリックして選択します。

### 画像12：ポリゴンツールを選択する

（ここに画像12「ポリゴンツールを選択する.png」を挿入）

アノテーションを行う画像を選択したら、次にアノテーションに使用するツールを選びます。今回は顔の「輪郭」に沿って精密にアノテーションを行いたいため、「ポリゴンツール（Polygon Tool）」を使用します。ポリゴンツールは、複数の点をクリックして多角形を描くことで、複雑な形状のオブジェクトも囲むことができるツールです。

画面の右側や上部に、様々なアノテーションツールがアイコンで表示されているはずです。上の画像内のメモ書きで示されているように、ポリゴンツールのアイコン（多くの場合、角ばった多角形や鉛筆のようなアイコンです）を探してクリックし、選択してください。

### 画像13：アノテーション＆クラス決め＆TRAINに割り振り

（ここに画像13「アノテーション＆クラス決め＆TRAINに割り振り.png」を挿入）

ポリゴンツールを選択したら、いよいよ画像内の顔にアノテーションを施していきます。上の画像は、アノテーション作業中の画面の一例と、重要なポイントを示しています。

1.  **顔の輪郭に沿って点を打っていく**: マウスを使って、検出したい顔の輪郭に沿って、細かく点をクリックしていきます。点をたくさん打つほど、より正確な輪郭で顔を囲むことができます。最初の点をクリックした後、次の点をクリックすると線が引かれ、これを繰り返して顔全体を囲みます。最後に最初の点をクリックするか、Enterキーを押すなどすると、ポリゴンが完成します。
2.  **今回は顔の検出なので「face」と入力（手の場合は「hand」など）**: ポリゴンで顔を囲むと、多くの場合、その囲んだ領域が何であるかを示す「クラス名」を入力するポップアップウィンドウが表示されます。今回は顔を検出したいので、クラス名として「face」と入力します。もし手の検出を行いたい場合は「hand」と入力するなど、検出対象に応じて適切なクラス名を設定します。クラス名は、モデルが何を学習すべきかを定義する重要な情報です。
3.  **最後にSaveを押す**: クラス名を入力したら、「Save」や「保存」、「Create」といったボタンをクリックして、アノテーションを保存します。これで、1つの顔に対するアノテーションが完了です。1枚の画像に複数の顔がある場合は、それぞれの顔に対して同様の作業を繰り返します。
4.  **もしこのように「TEST」になっていたら「TRAIN」に変更する**: 画像の右上に、現在編集中の画像が「TRAIN（学習用）」「VALID（検証用）」「TEST（テスト用）」のどれに割り当てられているかを示す表示がある場合があります。アノテーション作業中は、基本的に画像を「TRAIN」に割り当てるようにしましょう。もし「TEST」や「VALID」になっている場合は、クリックして「TRAIN」に変更してください。データセットの分割については後ほど詳しく説明します。

### 画像14：次の画像に移動

（ここに画像14「次の画像に移動.png」を挿入）

1枚の画像に対するアノテーション作業がすべて完了したら、次の画像に進みます。アノテーションエディタの上部や下部に、次の画像や前の画像に移動するための矢印ボタン（「>」や「<」など）があるはずです。上の画像内のメモ書きの指示通り、右向きの矢印ボタンをクリックして、次の画像に移動し、同様にアノテーション作業を続けていきましょう。アップロードしたすべての画像に対して、丁寧にアノテーションを行ってください。


## 4. データセット作成とダウンロード：モデル学習の準備を整える

すべてのアノテーション作業が完了したら、次はいよいよ学習用データセットを作成し、それをローカル環境にダウンロードします。このデータセットが、YOLOモデルが「顔とは何か」を学習するための教科書となります。

### 画像15：データセットへの追加

（ここに画像15「データセットへの追加.png」を挿入）

すべてのアノテーション済み画像を確認し終えたら、これらの画像をデータセットに追加します。上の画像は、アノテーションが完了した画像の一覧画面を示しています。

1.  **ここを押す**: まず、画面上部にある「Annotated」や「アノテーション済み」といったタブまたはフィルターが選択されていることを確認します。これにより、アノテーションが完了した画像のみが表示されます。
2.  **次にここを押す**: アノテーション済みの画像がすべて表示されたら、それらをデータセットに追加するためのボタン（例：「Add X Images to Dataset」や「データセットに追加」）をクリックします。Xにはアノテーション済みの画像の枚数が入ります。

### 画像16：Train＆Validの比率の選択

（ここに画像16「Train＆Validの比率の選択.png」を挿入）

データセットに追加する画像を選択すると、次にその画像をどのように分割するかを設定する画面が表示されます。機械学習モデルを訓練する際には、一般的にデータを「学習用（Train）」「検証用（Valid）」「テスト用（Test）」の3つに分割します。

*   **学習用（Train）**: モデルが直接学習するために使用されるデータです。
*   **検証用（Valid）**: 学習中にモデルの性能を評価し、過学習（学習データに過剰に適合しすぎて、未知のデータに対する性能が低下すること）を防いだり、ハイパーパラメータ（学習率など）を調整したりするために使用されます。
*   **テスト用（Test）**: 学習が完了したモデルの最終的な性能を評価するために使用される、完全に未知のデータです。

上の画像は、このデータ分割の比率を設定する画面です。今回は、以下のように設定しましょう。

1.  **表示されている項目選択肢の中から選択**: データ分割の方法を選択します。通常、「Split Images Between Train/Valid/Test」やそれに類する項目がデフォルトで選択されているはずです。
2.  **Train：80%、Valid：20%になるように変更**: スライダーを操作するか、数値を直接入力して、学習用データ（Train）の割合を80%、検証用データ（Valid）の割合を20%に設定します。テスト用データ（Test）は今回は0%で構いません。これは、比較的小規模なデータセットで、まずは基本的なモデルを作成することを目的としているためです。より大規模なプロジェクトでは、テスト用データも適切に割り当てることが重要です。
3.  **最後にここを押す**: 比率の設定が完了したら、「Add Images」や「画像を追加」といったボタンをクリックして、データセットへの画像の追加と分割を確定します。

### 画像17：新しいデータセットの作成準備

（ここに画像17「新しいデータセットの作成準備.png」を挿入）

画像の追加と分割が完了すると、データセットのバージョン管理画面に遷移することがあります。Roboflowでは、データセットに対する変更（アノテーションの修正、画像の追加・削除、前処理の変更など）をバージョンとして管理できます。これにより、過去のデータセットの状態に戻したり、異なるバージョンのデータセットでモデルの性能を比較したりすることが容易になります。

上の画像は、新しいデータセットバージョンを作成するための準備画面です。

1.  **ここを押す**: まず、左側のメニューから「Dataset」や「データセット」といった項目が選択されていることを確認します。
2.  **次にここを押す**: 画面上部にある「New Dataset Version」や「新しいデータセットバージョンを作成」といったボタンをクリックして、新しいバージョンの作成プロセスを開始します。

### 画像18：データセット作成準備（プリプロセッシング確認）

（ここに画像18「データセット作成準備.png」を挿入）

新しいデータセットバージョンを作成する過程で、いくつかの設定ステップがあります。最初のステップは「Preprocessing（前処理）」です。前処理とは、モデルの学習効率や性能を向上させるために、元画像に対して行われる様々な変換処理のことです。例えば、画像の向きを自動で修正したり、画像のサイズを統一したりします。

上の画像は、前処理の設定画面です。

1.  **この画面と同じ表示になっていることを確認**: 通常、Roboflowはプロジェクトの種類やデータの内容に応じて、推奨される前処理ステップを自動的に提案してくれます。「Auto-Orient（自動方向修正）」や「Resize（リサイズ、例：Stretch to 640x640）」といった項目が設定されていることを確認しましょう。特に変更する必要がなければ、デフォルト設定のままで問題ありません。
2.  **次にここを押す**: 設定内容を確認したら、「Continue」や「続行」といったボタンをクリックして、次のステップに進みます。

### 画像19：データセット作成準備２（Augmentation設定）

（ここに画像19「データセット作成準備２.png」を挿入）

前処理の次のステップは「Augmentation（データ拡張）」です。データ拡張とは、既存の学習データにランダムな変換（回転、反転、明るさ調整など）を加えることで、学習データのバリエーションを人工的に増やし、モデルの汎化性能（未知のデータに対する適応能力）を高めるテクニックです。特に、学習データが少ない場合に有効です。

上の画像は、データ拡張の設定画面です。

1.  **ここを押す**: 今回は、まずは基本的なモデル作成を目指すため、データ拡張は行わずに進めます。「Continue」や「続行」といったボタンをクリックして、このステップをスキップ（またはデフォルトのオフ状態で）次に進みましょう。より高度なモデル作成を目指す際には、ここで様々な拡張手法を試してみるのも良いでしょう。

### 画像20：データセット作成準備３（データセット作成実行）

（ここに画像20「データセット作成準備３.png」を挿入）

前処理とデータ拡張の設定（今回は拡張なし）が完了すると、いよいよデータセットバージョンを作成する最終ステップです。

上の画像は、これまでの設定内容の確認と、データセット作成を実行するための画面です。

1.  **ここを押す**: 「Create」や「作成」といったボタンをクリックすると、Roboflowがこれまでの設定に基づいてデータセットの生成を開始します。画像の枚数や処理内容によっては、少し時間がかかる場合があります。

### 画像21：データセットのダウンロード

（ここに画像21「データセットのダウンロード.png」を挿入）

データセットの生成が完了すると、上の画像のような画面が表示されます。

1.  **新しいデータセットが作成されていることを確認**: 画面左側や中央に、新しく作成されたデータセットのバージョン情報（例：「v1 - 2025-05-09 11:30am」など）が表示されていることを確認します。また、Train/Validの画像の枚数なども確認できます。
2.  **ここを押す**: 作成されたデータセットをローカル環境にダウンロードするために、「Download Dataset」や「データセットをダウンロード」といったボタンをクリックします。

### 画像22：ZIP形式でデータセットをダウンロード

（ここに画像22「ZIP形式でデータセットをダウンロード.png」を挿入）

データセットのダウンロードボタンをクリックすると、ダウンロード形式を選択するポップアップウィンドウが表示されます。YOLOモデルのトレーニングに使用するためには、特定のフォーマットでデータセットをエクスポートする必要があります。

上の画像は、ダウンロード形式の選択画面です。

1.  **Formatから「YOLOv11」（または類似のYOLO形式）を選択**: 「Format」というドロップダウンリストから、お使いのYOLOのバージョンに合った形式を選択します。リポジトリのファイル名から「yolov11n_seg」とあるため、「YOLOv11」や、もしなければ「YOLOv8」、「YOLOv5」など、YOLO系のセグメンテーション用フォーマットを選択してください。もし「YOLOv11」という選択肢が具体的にない場合は、リポジトリの`train_yolov11n_seg.bat`や`config.yaml`で指定されているモデルアーキテクチャ（例：YOLOv8-seg, YOLOv5-segなど）に対応する形式を選びます。このチュートリアルでは、リポジトリの構成からYOLOv11（またはそれに準ずるYOLO形式）を想定しています。
2.  **ZIP形式でのダウンロードを選択**: ダウンロードオプションとして、「Download zip to computer」や「ZIP形式でダウンロード」といった、ZIPファイルとして一括ダウンロードする選択肢を選びます。
3.  **最後にここを押す**: フォーマットとダウンロードオプションを選択したら、「Continue」や「続行」、「Download」といったボタンをクリックします。すると、データセットがZIPファイルとしてお使いのコンピュータのダウンロードフォルダに保存されます。

これで、顔検出モデルをトレーニングするためのデータセットの準備とダウンロードが完了しました。次の章では、ダウンロードしたデータセットを使って、いよいよローカル環境でYOLOモデルのトレーニングを行います。


## 5. ローカル環境でのセットアップとモデルトレーニング：自分だけの顔検出モデルを鍛え上げる

Roboflowから顔検出用のデータセットをダウンロードしたら、次はいよいよあなたのコンピュータ（ローカル環境）で、YOLOモデルのトレーニングを行います。このステップで、ダウンロードしたデータセットを使って、AIに「顔とはどのようなものか」を学習させ、あなた専用の顔検出モデルを構築します。

### 画像23：ZIPフォルダの展開

（ここに画像23「ZIPフォルダの展開.png」を挿入）

まず、RoboflowからダウンロードしたデータセットのZIPファイルを展開（解凍）します。お使いのコンピュータのOS（Windows、macOSなど）に標準で備わっている展開機能や、専用の解凍ソフト（例：7-Zip、WinRARなど）を使って、ZIPファイルの中身を取り出してください。上の画像は、Windows環境でZIPファイルを右クリックして「すべて展開」を選択する様子を示しています。展開先は、わかりやすい場所（例：デスクトップやドキュメントフォルダなど）を指定しましょう。

### 画像24：展開後のフォルダをdatasetに変更

（ここに画像24「展開後のフォルダをdatasetに変更.png」を挿入）

ZIPファイルを展開すると、データセット名のフォルダ（例：「FaceDetection-1」など、Roboflowで設定したプロジェクト名とバージョン番号に基づいた名前）が作成されます。このフォルダ名を、後の作業で使いやすくするために「**dataset**」という名前に変更します。上の画像のように、展開されたフォルダを選択し、名前を変更してください。

### 画像25：フォルダ構成＆datasetの入れ替え と 画像31：train-validのデータセットの入れ替え作業

（ここに画像25「フォルダ構成＆datasetの入れ替え.png」を挿入）
（ここに画像31「train-validのデータセットの入れ替え作業.png」を挿入）

次に、この「dataset」フォルダを、GitHubリポジトリからクローンした「training_folder」ディレクトリの中に配置します。

**重要な注意点（画像31より）：**
リポジトリの`training_folder`内にもともと`dataset`という名前のフォルダが存在し、その中に`train`フォルダや`valid`フォルダが含まれている場合があります。これはサンプルデータやテンプレート構造かもしれません。

**作業手順：**
1.  **Roboflowからダウンロードし、展開・リネームした「dataset」フォルダを開きます。** この中には、`train`フォルダ、`valid`フォルダ、そして`data.yaml`という設定ファイルなどが入っているはずです。
2.  **GitHubリポジトリからクローンした`yolo_tips_content`フォルダの中にある`training_folder`を開きます。**
3.  **`training_folder`の中に、先ほど準備した「dataset」フォルダ全体をコピーまたは移動します。**
    *   もし`training_folder`内に既に`dataset`フォルダが存在し、その中に古い`train`や`valid`フォルダがある場合は、**Roboflowからダウンロードした新しい`train`フォルダと`valid`フォルダで、既存のものを置き換えてください。** 画像31の指示にあるように、「このフォルダの中にある `train` `valid` 2つのフォルダをダウンロードしたフォルダの中にある同名フォルダと入れ替える」という作業を行います。つまり、Roboflowから取得した最新の学習データと検証データを使用するようにします。
    *   `data.yaml`ファイルもRoboflowからダウンロードしたものを使用します。これはデータセットのパスやクラス情報をYOLOモデルに伝える重要なファイルです。

最終的に、`training_folder`ディレクトリの直下に`dataset`フォルダがあり、その`dataset`フォルダの中にRoboflowからダウンロードした`train`（画像とラベルを含む）、`valid`（画像とラベルを含む）、そして`data.yaml`などが配置されている状態になればOKです。画像25はWindows環境でのフォルダ構成例を示しています。

### Windows環境でのPython環境構築の準備

モデルのトレーニングや後述するモザイク処理プログラムを実行するためには、Pythonとその関連ライブラリ（プログラム部品）が必要です。リポジトリには、これらの環境を簡単にセットアップするためのバッチファイル（`.bat`拡張子のファイル）が含まれています。

**補足：Pythonのインストール**
もし、お使いのWindows環境にPythonがインストールされていない場合は、事前にPython公式サイトからインストーラをダウンロードし、インストールしておく必要があります。その際、「Add Python to PATH」のチェックボックスをオンにすることを忘れないでください。YOLOv11やUltralyticsライブラリは比較的新しいPythonバージョンを要求することがあるため、Python 3.8以上（3.10や3.11を推奨）をインストールすると良いでしょう。

### 画像26：環境設定ファイルの起動 と 画像29：仮装環境作成

（ここに画像26「環境設定ファイルの起動.png」を挿入）
（ここに画像29「仮装環境作成.png」を挿入）

`training_folder`の中に、「`setup_environment.bat`」というファイルがあります。これが、Pythonの仮想環境を作成し、必要なライブラリをインストールしてくれるスクリプトです。

1.  **`setup_environment.bat`をダブルクリックして実行します。**
    すると、コマンドプロンプト（黒い画面）が起動し、自動的に処理が始まります。画像29は、このスクリプトが実行されている途中の様子を示しています。「Requirement already satisfied」というメッセージが多く表示されることがありますが、これは必要なライブラリが既に存在しているか、依存関係で解決されていることを示しています。初めて実行する場合は、ライブラリのダウンロードとインストールが行われるため、少し時間がかかることがあります。
2.  **「Environment setup complete!」と表示されるまで待機します。**
    画像29の下部にあるように、「Environment setup complete!」というメッセージが表示され、「続行するには何かキーを押してください...」と表示されたら、環境構築は成功です。
3.  **Enterキーを押してコマンドプロンプトを閉じます。**

このスクリプトは、`venv`という名前のPython仮想環境を`training_folder`内に作成し、`ultralytics`（YOLOの実行に必要）や`opencv-python`（画像処理に必要）などのライブラリをその仮想環境内にインストールします。仮想環境を使うことで、プロジェクトごとに異なるバージョンのライブラリを管理でき、他のPython環境との衝突を避けることができます。

### `config.yaml`ファイルの詳細解説

`training_folder`の中には、「`config.yaml`」というファイルがあります。これは、YOLOモデルのトレーニング設定を記述する構成ファイルです。中身をテキストエディタで開いて確認してみましょう。

```yaml
# YOLOv11n-seg Training Configuration

# Model parameters
model: yolo11n-seg.pt  # ベースとする事前学習済みモデル。yolov11n-seg.ptは軽量なセグメンテーションモデルです。
epochs: 100            # 学習を繰り返す回数（エポック数）。データセットの規模や複雑さに応じて調整します。
imgsz: 640             # 学習に使用する画像のサイズ。640x640ピクセルにリサイズされます。
batch: 4               # 一度に処理する画像の枚数（バッチサイズ）。GPUメモリに応じて調整します。
workers: 4             # データ読み込みに使用するCPUコア数。環境に合わせて調整します。
device: 0              # 使用するGPUのデバイス番号。0は通常、最初のGPUを指します。CPUのみの場合は 'cpu' と指定します。

# Dataset
data: ./dataset/data.yaml # データセットの構成ファイル（Roboflowからダウンロードしたもの）へのパス。

# Output
name: yolov11n_seg_custom_model # 学習結果が保存されるフォルダ名。
project: runs/segment          # 学習結果が保存されるプロジェクトディレクトリ。
```

*   **`model`**: トレーニングのベースとなるモデルを指定します。`yolo11n-seg.pt`は、YOLOv11のnanoバージョンで、セグメンテーション（物体の輪郭検出）に対応した軽量モデルです。`.pt`ファイルはPyTorch形式の学習済みモデルファイルです。
*   **`epochs`**: 学習データセット全体を何回繰り返して学習するかを指定します。エポック数が多いほど学習が進みますが、多すぎると過学習のリスクもあります。通常は数十から数百の範囲で設定されます。
*   **`imgsz`**: 入力画像をどのサイズにリサイズして学習に使用するかを指定します。YOLOモデルは特定の入力サイズを期待することが多いです。
*   **`batch`**: 一度の計算でモデルに渡す画像の枚数です。バッチサイズが大きいほど学習が安定する傾向がありますが、GPUのメモリ容量を多く消費します。メモリ不足エラーが出る場合は、この値を小さくします。
*   **`workers`**: データ読み込み処理を並列で行うためのワーカースレッド数です。CPUのコア数に応じて設定します。
*   **`device`**: 学習に使用するデバイスを指定します。NVIDIAのGPUが利用可能な場合は`0`（1番目のGPU）、`1`（2番目のGPU）のように指定します。GPUがない場合は`cpu`と指定します。
*   **`data`**: データセットの構成ファイルである`data.yaml`へのパスを指定します。この`data.yaml`は、Roboflowからダウンロードした`dataset`フォルダ内に含まれており、学習用・検証用画像のパスやクラス名などが定義されています。
*   **`name`**: 学習結果（学習済みモデルの重みファイルやログなど）が保存されるフォルダの名前です。
*   **`project`**: `name`で指定したフォルダが作成される親ディレクトリです。

これらのパラメータは、必要に応じて調整することができますが、まずはデフォルトの設定でトレーニングを実行してみましょう。

### 画像27：トレーニング用のファイルの起動 と 画像30：モデルトレーニング

（ここに画像27「トレーニング用のファイルの起動.png」を挿入）
（ここに画像30「モデルトレーニング.png」を挿入）

環境設定とデータセットの準備が整ったら、いよいよモデルのトレーニングを開始します。`training_folder`の中に、「`train_yolov11n_seg.bat`」というファイルがあります。これが、`config.yaml`の設定に基づいてYOLOモデルのトレーニングを実行するスクリプトです。

1.  **`train_yolov11n_seg.bat`をダブルクリックして実行します。**
    コマンドプロンプトが起動し、トレーニングプロセスが開始されます。画像30は、トレーニングが進行している様子を示しています。GPUが利用可能な場合は、GPUを使って高速に学習が行われます。
    画面には、エポックごとの学習状況（損失関数の値、精度など）が表示されます。
    ```
    Epoch   gpu_mem box_loss seg_loss cls_loss dfl_loss Instances Size
    1/100   0.506G   0.8321   0.6012    2.543   0.9876         2  640: 100% ...
    ... (中略) ...
    Epoch   gpu_mem box_loss seg_loss cls_loss dfl_loss Instances Size
    100/100 0.506G   0.3569   0.4097     1.81   0.8024         2  640: 100% ...
    ```
    これらの数値は、モデルがどれだけうまく学習できているかを示す指標です。通常、`loss`（損失）の値はエポックが進むにつれて小さくなっていきます。
2.  **トレーニングが完了するまで待ちます。**
    `config.yaml`で指定されたエポック数（デフォルトでは100エポック）の学習が完了するまでには、コンピュータの性能やデータセットのサイズによって数分から数時間かかることがあります。画像30では、100エポックの学習が完了した後のメッセージが表示されています。
    ```
    100 epochs completed in 0.012 hours.
    Optimizer stripped from runs/segment/yolov11n_seg_custom_modelX/weights/last.pt, 6.0MB
    Optimizer stripped from runs/segment/yolov11n_seg_custom_modelX/weights/best.pt, 6.0MB
    Validating runs/segment/yolov11n_seg_custom_modelX/weights/best.pt...
    (中略)
    Results saved to runs/segment/yolov11n_seg_custom_modelX
    Learn more at https://docs.ultralytics.com/modes/train
    Training completed.
    ```
    `yolov11n_seg_custom_modelX` の `X` の部分は実行ごとに数字が変わることがあります。
3.  **「続行するには何かキーを押してください...」と表示されたら、Enterキーを押してコマンドプロンプトを閉じます。**

トレーニングが完了すると、`training_folder`内の`runs/segment/yolov11n_seg_custom_modelX`（または`config.yaml`で指定した`project`/`name`）というフォルダの中に、学習結果が保存されます。特に重要なのは、`weights`フォルダの中にある`best.pt`というファイルです。これが、検証用データセットで最も良い性能を示した学習済みモデルの重みファイルであり、後の顔モザイク処理プログラムで使用します。

これで、あなただけのカスタム顔検出モデルのトレーニングが完了しました！次の章では、この学習済みモデルを使って、実際に画像から顔を検出し、モザイクをかけるプログラムを実行してみましょう。


## 6. 顔モザイクプログラムの実行：学習済みモデルで顔を隠そう！

カスタム顔検出モデルのトレーニングが完了し、`best.pt`という学習済みモデルファイルを手に入れました。いよいよ最後のステップ、このモデルを使って実際に画像内の顔を検出し、モザイクをかけるプログラムを実行してみましょう。

### 画像28：モザイクプログラムの起動

（ここに画像28「モザイクプログラムの起動.png」を挿入）

`training_folder`の中に、「`detect_mosaic.bat`」というファイルがあります。これが、学習済みのYOLOモデル（`best.pt`）を使い、指定されたフォルダ内の画像に含まれる顔を検出し、それらにモザイク処理を施して別のフォルダに保存するPythonプログラム（`detect.py`）を実行するためのバッチファイルです。

1.  **`detect_mosaic.bat`をダブルクリックして実行します。**
    コマンドプロンプトが起動し、Pythonプログラム`detect.py`が実行されます。

### `detect.py`プログラムの詳細解説

`detect_mosaic.bat`を実行すると、内部的には`python detect.py`というコマンドが実行され、`detect.py`スクリプトが動作します。このスクリプトは、ユーザーにいくつかの情報を尋ね、それに基づいて処理を行います。

#### プログラムの機能概要

`detect.py`は、以下の機能を持っています。

*   指定されたフォルダ内にあるすべての画像ファイル（.jpg, .jpeg, .png, .bmp, .tiff）を読み込みます。
*   トレーニング済みのYOLOv11n-segモデル（`runs/segment/yolov11n_seg_custom_modelX/weights/best.pt`）を使用して、各画像から顔の領域を検出（セグメンテーション）します。
*   検出された顔の領域に対して、指定されたサイズのモザイク処理を適用します。
*   モザイク処理が施された画像を、`runs/segment/mosaic_results`というフォルダに保存します。元画像と同じファイル名で、先頭に`mosaic_`という接頭辞が付きます。
*   もしモデルが見つからない場合は、代替パスの入力を促します。
*   GPUが利用可能であればGPUを、そうでなければCPUを使用して推論処理を行います。

#### ユーザー入力部分の説明

`detect.py`を実行すると、コマンドプロンプトで以下の情報を順番に入力するよう求められます。

1.  **Enter folder path containing images to process:**
    モザイク処理を施したい画像が保存されているフォルダのパスを入力します。例えば、デスクトップにある「TestImages」というフォルダを指定したい場合は、そのフルパス（例：`C:\Users\YourUserName\Desktop\TestImages`）を入力します。リポジトリ内の`training_folder`の兄弟ディレクトリとして`annotation_test_images`というフォルダが提供されているので、これを使ってテストすることができます。その場合のパスは、`training_folder`からの相対パスで`../annotation_test_images`のように指定するか、フルパスで指定します。

2.  **Enter mosaic pixel size (smaller = more pixelated, recommended 8-20):**
    モザイクの粗さを決定するピクセルサイズを入力します。この値が小さいほど、モザイクはより細かく（ピクセルが小さく）なり、元の顔が分かりにくくなります。逆に大きいと、モザイクは粗くなります。推奨値は8から20の間です。例えば、「10」と入力します。不正な値を入力した場合は、デフォルトで10が使用されます。

3.  **Enter detection confidence threshold (0.1-1.0, default=0.25):**
    顔検出の信頼度の閾値を入力します。モデルが顔を検出した際に、その確からしさ（信頼度スコア）がこの閾値以上である場合にのみ、モザイク処理の対象となります。値は0.1から1.0の間で設定します。例えば、「0.25」と入力すると、25%以上の信頼度で検出された顔にモザイクがかかります。この値を高くすると、誤検出は減りますが、検出漏れが増える可能性があります。低くすると、より多くの顔を検出しようとしますが、顔でないものを誤って検出する可能性も高まります。不正な値を入力した場合は、デフォルトで0.25が使用されます。

#### 各入力値の推奨値や設定方法の具体例

*   **画像フォルダパス**: まずはリポジトリ内の`annotation_test_images`フォルダを使ってみましょう。`training_folder`から見た相対パスは `../annotation_test_images` です。もし`training_folder`が `C:\MyProjects\yolo_tips_content\training_folder` にあるなら、テスト画像フォルダのフルパスは `C:\MyProjects\yolo_tips_content\annotation_test_images` となります。
*   **モザイクピクセルサイズ**: 「10」や「15」あたりから試してみて、結果を見ながら調整するのが良いでしょう。
*   **信頼度閾値**: デフォルトの「0.25」で試してみて、もし顔が検出されすぎる（顔でないものまで検出する）場合は少し値を大きく（例：0.4）、逆に顔の検出漏れが多い場合は少し値を小さく（例：0.15）してみると良いかもしれません。

#### プログラムの主要な処理の流れ

1.  学習済みモデル（`best.pt`）をロードします。
2.  指定された画像フォルダから画像ファイルを1枚ずつ読み込みます。
3.  読み込んだ画像に対してモデルを実行し、顔のセグメンテーションマスク（顔の領域を示すデータ）を取得します。
4.  取得した各セグメンテーションマスクに対して、`apply_mosaic_to_mask`関数を使ってモザイク処理を適用します。
    *   この関数は、マスク領域を一度縮小し、再度元のサイズに拡大することでモザイク効果を生み出します。
5.  モザイク処理後の画像を`runs/segment/mosaic_results`フォルダに保存します。
6.  すべての画像の処理が完了したら、その旨を通知してプログラムを終了します。

#### 出力結果（`mosaic_results`フォルダ）の説明

処理が完了すると、`training_folder`内の`runs/segment/mosaic_results`というフォルダが作成され、その中にモザイク処理済みの画像が保存されます。ファイル名は、元の画像ファイル名の先頭に`mosaic_`が付加されたものになります（例：元の画像が`image1.jpg`なら、出力は`mosaic_image1.jpg`）。

### `annotation_test_images`フォルダの画像の利用方法

GitHubリポジトリのルートディレクトリ（`yolo_tips_content`）には、`annotation_test_images`というフォルダが含まれています。このフォルダには、アノテーション作業や、トレーニング済みモデルのテストに使用できるサンプル画像がいくつか入っているはずです。

顔モザイクプログラムを実行する際に、処理対象の画像フォルダとしてこの`annotation_test_images`フォルダのパスを指定することで、手軽にプログラムの動作確認や、作成した顔検出モデルの性能テストを行うことができます。

例えば、`detect.py`実行時に「Enter folder path containing images to process:」と聞かれたら、`training_folder`から見た相対パスで `../annotation_test_images` と入力するか、このフォルダの絶対パスを入力してください。

これで、YOLOを使った顔検出とモザイク処理の一連の流れを体験することができました！

